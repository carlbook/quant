Uploaded to github for QIM in case you're curious. I found CRSP data on kaggle and after repackaging the sas7bdat files to sqlite, my idea was to create embedding vectors and use the Vaswani transformer encoder followed by a fully-connected network to estimate one of several training signals. I started in Keras but eventually switched to Pytorch which I now prefer. Early on I knowingly did not do true roll-forward cross validation because I wasn't sure how to make multiple passes over the data in this context. Initial results showed an improving validation loss, but eventually I realized it was because my model was memorizing the S&P500 and its generally positive correlation with individual stocks. I tried several dozen models with other ideas and data preparation (volatility normalization, wavelet as dimensionality reduction, etc) but all they would do is overfit the training data, not generalize. My hunch is that there are transient windows of predictability in equities surrounded by a lot of noise. I stopped working on all this due to the time commitment of my regular job but someday I hope to come back and reconsider the problem in terms of ideas from dynamical systems (tipping points, criticality) and possibly some explicit probabilistic modeling. Getting access to low-cost or free data beyond daily OHLCV is a challenge, but I think it would be useful because markets are at times driven by external forcing. 
